{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"AWS IoT Core Configuration: 'AWS_IoT.md' Raspberry Pi with Humidity and Temperature Sensor Configuration: 'RaPi_with_Sensor.md'","title":"Home"},{"location":"AWS_IoT/","text":"Appendix - 1: AWS IoT Platform Configuration 1. What is AWS IoT Platform? AWS IoT provides broad and deep functionality, spanning the edge to the cloud, so you can build IoT solutions for virtually any use case across a wide range of devices. Since AWS IoT integrates with AI services, you can make devices smarter, even without Internet connectivity. Built on the AWS cloud, used by millions of customers in 190 countries, AWS IoT can easily scale as your device fleet grows and your business requirements evolve. AWS IoT also offers the most comprehensive security features so you can create preventative security policies and respond immediately to potential security issues. 2. What Config Do We Need for this Lab? For this lab we are going to use the MQTT Broker service from AWS IoT Core, which would be used by the IoT Sensor devices (Raspberry Pi with DHT22 sensor in our case) to publish the sensor data. As the direct MQTT access is not permitted by AWS, we would have to create certificate and policies to allow our sensor devices to publish the sensor data on MQTT Broker. This article would explain the process for the same. 3. Prerequisites: You need to have a valid AWS Account with Administrative access to AWS IoT Core. 4. Create new MQTT Policies: Login to AWS console and open AWS IoT Core page. 4.1 Create New Policies: Click on the \" Create \" button on \" Secure -- Policies \" page to create a new policy as shown in the following screenshot - Note: If you are creating a Policy for the first time, you may see a different screen with \" Create a policy \" button. Just click on that button and follow other steps. 4.2 Create MQTT_Connect Policy: Create a new MQTT_Connect policity with \" iot:Connect \" action, \" * \" Resource ARN and \" Allow \" Effect as shown in the following screenshot - 4.3 Create MQTT_Publish Policy: Create a new MQTT_Publish policity with \" iot:Publish \" action, \" * \" Resource ARN and \" Allow \" Effect as shown in the following screenshot - 4.4 Create MQTT_Subscribe Policy: Create a new MQTT_Subscribe policity with \" iot:Subscribe \" action, \" * \" Resource ARN and \" Allow \" Effect as shown in the following screenshot - 4.4 Check the Policies Page: Your Policies page should look similar to the following screenshot - 5. Create and Download new Certificate: 5.1 Create New Policies: Click on the \" Create \" button on \" Secure -- Certificates \" page to create a new certificate as shown in the following screenshot - Note: If you are creating a Certificate for the first time, you may see a different screen with \" Create a certificate \" button. Just click on that button and follow other steps. 5.2 Create One-Click-Certificate: Click on the \" Create certificate \" button to create a new One-click-certificate as shown in the following screenshot - 5.3 Download Certificates: Click on the \" Download \" buttons and download the Certificate file, Public key, and Private key file as shown in the following screenshot - 5.4 Close the Create Certificate Page: Close the \"Create Certificate\" page by clicking on the \" Done \" button. 5.4 Download Amazon Root CA Certificate: Save the \" RSA 2048 bit key: Amazon Root CA 1 \" file from the following page as \" AmazonRootCA.crt \" - (Page URL - https://docs.aws.amazon.com/iot/latest/developerguide/managing-device-certs.html#server-authentication ) Note: Save all these Certificate and Key files on your machine as you would need to upload them on Raspberry Pi (Appendix-2) and on MQTT_to_DB Agent. 5.5 Check the Certificates Page: Now on the \" Secure -- Certificates \" page you should see a new certificate. Note that your certificate is still inactive. 6. Activate the Certificate: Navigate to \" Secure -- Certificates \"; click on the Certificate options and select \" Activate \" to activate the certificate as shown in following screenshot - 7. Associate the Policies with Certificate: Navigate to \" Secure -- Certificates \" and click on the certificate to modify the certificate properties. On this certificate properties page click on \" Policies \" and from the \" Actions \" menu select \" Attach Policy \" as shown in the following screenshot - Select all the policies and click on \" Attach \" button as shown in the following screenshot - Your certificate Policies section should look like this - 8. Locate Custom Endpoint: You would need Custom endpoint to connect to the AWS IoT platform. It will be used by the MQTT clients as MQTT Broker host. You can locate your Customer Endpoint under \"Settings\" section as shown in the following screenshot -","title":"AWS IoT Core Configuration"},{"location":"AWS_IoT/#appendix-1-aws-iot-platform-configuration","text":"","title":"Appendix - 1: AWS IoT Platform Configuration"},{"location":"AWS_IoT/#1-what-is-aws-iot-platform","text":"AWS IoT provides broad and deep functionality, spanning the edge to the cloud, so you can build IoT solutions for virtually any use case across a wide range of devices. Since AWS IoT integrates with AI services, you can make devices smarter, even without Internet connectivity. Built on the AWS cloud, used by millions of customers in 190 countries, AWS IoT can easily scale as your device fleet grows and your business requirements evolve. AWS IoT also offers the most comprehensive security features so you can create preventative security policies and respond immediately to potential security issues.","title":"1. What is AWS IoT Platform?"},{"location":"AWS_IoT/#2-what-config-do-we-need-for-this-lab","text":"For this lab we are going to use the MQTT Broker service from AWS IoT Core, which would be used by the IoT Sensor devices (Raspberry Pi with DHT22 sensor in our case) to publish the sensor data. As the direct MQTT access is not permitted by AWS, we would have to create certificate and policies to allow our sensor devices to publish the sensor data on MQTT Broker. This article would explain the process for the same.","title":"2. What Config Do We Need for this Lab?"},{"location":"AWS_IoT/#3-prerequisites","text":"You need to have a valid AWS Account with Administrative access to AWS IoT Core.","title":"3. Prerequisites:"},{"location":"AWS_IoT/#4-create-new-mqtt-policies","text":"Login to AWS console and open AWS IoT Core page.","title":"4. Create new MQTT Policies:"},{"location":"AWS_IoT/#41-create-new-policies","text":"Click on the \" Create \" button on \" Secure -- Policies \" page to create a new policy as shown in the following screenshot - Note: If you are creating a Policy for the first time, you may see a different screen with \" Create a policy \" button. Just click on that button and follow other steps.","title":"4.1 Create New Policies:"},{"location":"AWS_IoT/#42-create-mqtt_connect-policy","text":"Create a new MQTT_Connect policity with \" iot:Connect \" action, \" * \" Resource ARN and \" Allow \" Effect as shown in the following screenshot -","title":"4.2 Create MQTT_Connect Policy:"},{"location":"AWS_IoT/#43-create-mqtt_publish-policy","text":"Create a new MQTT_Publish policity with \" iot:Publish \" action, \" * \" Resource ARN and \" Allow \" Effect as shown in the following screenshot -","title":"4.3 Create MQTT_Publish Policy:"},{"location":"AWS_IoT/#44-create-mqtt_subscribe-policy","text":"Create a new MQTT_Subscribe policity with \" iot:Subscribe \" action, \" * \" Resource ARN and \" Allow \" Effect as shown in the following screenshot -","title":"4.4 Create MQTT_Subscribe Policy:"},{"location":"AWS_IoT/#44-check-the-policies-page","text":"Your Policies page should look similar to the following screenshot -","title":"4.4 Check the Policies Page:"},{"location":"AWS_IoT/#5-create-and-download-new-certificate","text":"","title":"5. Create and Download new Certificate:"},{"location":"AWS_IoT/#51-create-new-policies","text":"Click on the \" Create \" button on \" Secure -- Certificates \" page to create a new certificate as shown in the following screenshot - Note: If you are creating a Certificate for the first time, you may see a different screen with \" Create a certificate \" button. Just click on that button and follow other steps.","title":"5.1 Create New Policies:"},{"location":"AWS_IoT/#52-create-one-click-certificate","text":"Click on the \" Create certificate \" button to create a new One-click-certificate as shown in the following screenshot -","title":"5.2 Create One-Click-Certificate:"},{"location":"AWS_IoT/#53-download-certificates","text":"Click on the \" Download \" buttons and download the Certificate file, Public key, and Private key file as shown in the following screenshot -","title":"5.3 Download Certificates:"},{"location":"AWS_IoT/#54-close-the-create-certificate-page","text":"Close the \"Create Certificate\" page by clicking on the \" Done \" button.","title":"5.4 Close the Create Certificate Page:"},{"location":"AWS_IoT/#54-download-amazon-root-ca-certificate","text":"Save the \" RSA 2048 bit key: Amazon Root CA 1 \" file from the following page as \" AmazonRootCA.crt \" - (Page URL - https://docs.aws.amazon.com/iot/latest/developerguide/managing-device-certs.html#server-authentication ) Note: Save all these Certificate and Key files on your machine as you would need to upload them on Raspberry Pi (Appendix-2) and on MQTT_to_DB Agent.","title":"5.4 Download Amazon Root CA Certificate:"},{"location":"AWS_IoT/#55-check-the-certificates-page","text":"Now on the \" Secure -- Certificates \" page you should see a new certificate. Note that your certificate is still inactive.","title":"5.5 Check the Certificates Page:"},{"location":"AWS_IoT/#6-activate-the-certificate","text":"Navigate to \" Secure -- Certificates \"; click on the Certificate options and select \" Activate \" to activate the certificate as shown in following screenshot -","title":"6. Activate the Certificate:"},{"location":"AWS_IoT/#7-associate-the-policies-with-certificate","text":"Navigate to \" Secure -- Certificates \" and click on the certificate to modify the certificate properties. On this certificate properties page click on \" Policies \" and from the \" Actions \" menu select \" Attach Policy \" as shown in the following screenshot - Select all the policies and click on \" Attach \" button as shown in the following screenshot - Your certificate Policies section should look like this -","title":"7. Associate the Policies with Certificate:"},{"location":"AWS_IoT/#8-locate-custom-endpoint","text":"You would need Custom endpoint to connect to the AWS IoT platform. It will be used by the MQTT clients as MQTT Broker host. You can locate your Customer Endpoint under \"Settings\" section as shown in the following screenshot -","title":"8. Locate Custom Endpoint:"},{"location":"RaPi_with_Sensor/","text":"Appendix - 2: Raspberry Pi with DHT22 Sensor and Docker Container 1. Hardware and Software Requirements: Following are the Hardware and Software prerequisites that you need before you begin with this article - 1.1 Hardware: Raspberry Pi with Power Adapter SD Card and SD Card Reader Ethernet Cable DHT22 Sensor 10K Ohm Resistor Jumper wires and Breadboard Keyboard and Monitor (Optional) 1.2 Software: Official Raspbian Image Etcher for flashing Raspbian OS image into theSD card 2. DHT22 Sensor Connection with Raspberry Pi You can connect the DHT22 sensor with Raspberry Pi as shown in the following image. Pin 1 and Pin 4 on the sensor should be connected to the 3.3V Power Pin and Ground Pin respectively. Pin 2 on the sensor should be connected to GPIO Pin 4. 3. Burn Raspbian OS on the SD Card and enable SSH Download the Rasbian OS and use [Etcher] (https://www.balena.io/etcher/) to install it on the SD card. For headless setup, open the SD Card on your computer, after the installation process gets completed to enable SSH. SSH can be enabled by placing a file named ssh, without any extension, onto the boot partition of the SD card from another computer. When the Pi boots, it looks for the ssh file. If it is found, SSH is enabled and the file is deleted. The content of the file does not matter; it could contain text, or nothing at all. If you have loaded Raspbian onto a blank SD card, you will have two partitions. The first one, which is the smaller one, is the boot partition. Place the file into this one. Now plug the SD card into your Raspberry Pi and power it on. 4. SSH into the Raspberry Pi and Install Docker Runtime SSH into your Raspberry Pi using the following command (replace with the IP Address assigned to your raspberry Pi) - ssh pi@ ip address On the password prompt you could use the default raspberry password - \" raspberry \" Now you should have access to the shell. Upgrade raspbian packages using the following command - sudo apt-get update sudo apt-get upgrade Install Docker runtime using the following command - curl -sSL https://get.docker.com | sh Add permission to Pi user to run Docker commands by adding \u201cpi\u201d user to \u201cdocker\u201d group using the following command \u2013 sudo usermod -aG docker pi You must Log off from Raspberry Pi and Login again, for this to take effect. Check Docker installation using the following command - docker --version If you see the correct version, you are good to go. 5. Download Required Files and Upload on Raspberry Pi: 5.1 Download the Certificate files from from AWS IoT core as described in Appendix - 1 5.2 Download the \" settings.ini \" from github repo - Link 5.3 Update the settings file variables accordingly. 5.4 Login to Raspberry Pi and create new settings folder under the 'pi' user's home directory, using the following command - mkdir /home/pi/settings 5.5 Upload the AWS Certificate Files and updated \"settings.ini\" file on Raspberry Pi using sftp - sftp pi@ ip-address cd /home/pi/settings put file-name Note: Make sure you copy the updated settings.ini, Amazon RootCA Certificate file, AWS IoT Core Things Certificate Private Key files into the '/home/pi/settings' directory on your Raspberry Pi. 5.6 Close the sftp session using the following command - bye 6. Test the Container on Raspberry Pi: Login back into the Raspberry Pi and run the sensor container in an interactive mode using the following command - sudo docker run --privileged -it -v /home/pi/settings:/usr/src/app/settings pradeesi/rapi_cl_hc_sensor 7. Configure Raspberry Pi to Trigger the Sensor Container at Startup: 7.1 open the 'rc.local' using the following command - sudo vi /etc/rc.local 7.2 Add following command at the top of this file (preferably in the first line) - sudo docker run --privileged -d -v /home/pi/settings:/usr/src/app/settings pradeesi/rapi_cl_hc_sensor 7.3 Save the file using ' Esc + :wq ' command on vi editor. 7.4 Change the execution bit using the following command - sudo chmod +x /etc/rc.local 7.5 Reboot the Raspberry Pi using ' sudo reboot '. 7.6 After the reboot process gets completed, login back to the Raspberry Pi and check if the sensor container is running using the command ' sudo docker ps '. Note: If the docker container is not getting triggered on system startup, add 5 second delay by adding ' sleep 5 ' before the docker command added in the 'rc.local' file (in Step# 7.2).","title":"Raspberry Pi with Humidity and Temperature Sensor Configuration"},{"location":"RaPi_with_Sensor/#appendix-2-raspberry-pi-with-dht22-sensor-and-docker-container","text":"","title":"Appendix - 2: Raspberry Pi with DHT22 Sensor and Docker Container"},{"location":"RaPi_with_Sensor/#1-hardware-and-software-requirements","text":"Following are the Hardware and Software prerequisites that you need before you begin with this article -","title":"1. Hardware and Software Requirements:"},{"location":"RaPi_with_Sensor/#11-hardware","text":"Raspberry Pi with Power Adapter SD Card and SD Card Reader Ethernet Cable DHT22 Sensor 10K Ohm Resistor Jumper wires and Breadboard Keyboard and Monitor (Optional)","title":"1.1 Hardware:"},{"location":"RaPi_with_Sensor/#12-software","text":"Official Raspbian Image Etcher for flashing Raspbian OS image into theSD card","title":"1.2 Software:"},{"location":"RaPi_with_Sensor/#2-dht22-sensor-connection-with-raspberry-pi","text":"You can connect the DHT22 sensor with Raspberry Pi as shown in the following image. Pin 1 and Pin 4 on the sensor should be connected to the 3.3V Power Pin and Ground Pin respectively. Pin 2 on the sensor should be connected to GPIO Pin 4.","title":"2. DHT22 Sensor Connection with Raspberry Pi"},{"location":"RaPi_with_Sensor/#3-burn-raspbian-os-on-the-sd-card-and-enable-ssh","text":"Download the Rasbian OS and use [Etcher] (https://www.balena.io/etcher/) to install it on the SD card. For headless setup, open the SD Card on your computer, after the installation process gets completed to enable SSH. SSH can be enabled by placing a file named ssh, without any extension, onto the boot partition of the SD card from another computer. When the Pi boots, it looks for the ssh file. If it is found, SSH is enabled and the file is deleted. The content of the file does not matter; it could contain text, or nothing at all. If you have loaded Raspbian onto a blank SD card, you will have two partitions. The first one, which is the smaller one, is the boot partition. Place the file into this one. Now plug the SD card into your Raspberry Pi and power it on.","title":"3. Burn Raspbian OS on the SD Card and enable SSH"},{"location":"RaPi_with_Sensor/#4-ssh-into-the-raspberry-pi-and-install-docker-runtime","text":"SSH into your Raspberry Pi using the following command (replace with the IP Address assigned to your raspberry Pi) - ssh pi@ ip address On the password prompt you could use the default raspberry password - \" raspberry \" Now you should have access to the shell. Upgrade raspbian packages using the following command - sudo apt-get update sudo apt-get upgrade Install Docker runtime using the following command - curl -sSL https://get.docker.com | sh Add permission to Pi user to run Docker commands by adding \u201cpi\u201d user to \u201cdocker\u201d group using the following command \u2013 sudo usermod -aG docker pi You must Log off from Raspberry Pi and Login again, for this to take effect. Check Docker installation using the following command - docker --version If you see the correct version, you are good to go.","title":"4. SSH into the Raspberry Pi and Install Docker Runtime"},{"location":"RaPi_with_Sensor/#5-download-required-files-and-upload-on-raspberry-pi","text":"5.1 Download the Certificate files from from AWS IoT core as described in Appendix - 1 5.2 Download the \" settings.ini \" from github repo - Link 5.3 Update the settings file variables accordingly. 5.4 Login to Raspberry Pi and create new settings folder under the 'pi' user's home directory, using the following command - mkdir /home/pi/settings 5.5 Upload the AWS Certificate Files and updated \"settings.ini\" file on Raspberry Pi using sftp - sftp pi@ ip-address cd /home/pi/settings put file-name Note: Make sure you copy the updated settings.ini, Amazon RootCA Certificate file, AWS IoT Core Things Certificate Private Key files into the '/home/pi/settings' directory on your Raspberry Pi. 5.6 Close the sftp session using the following command - bye","title":"5. Download Required Files and Upload on Raspberry Pi:"},{"location":"RaPi_with_Sensor/#6-test-the-container-on-raspberry-pi","text":"Login back into the Raspberry Pi and run the sensor container in an interactive mode using the following command - sudo docker run --privileged -it -v /home/pi/settings:/usr/src/app/settings pradeesi/rapi_cl_hc_sensor","title":"6. Test the Container on Raspberry Pi:"},{"location":"RaPi_with_Sensor/#7-configure-raspberry-pi-to-trigger-the-sensor-container-at-startup","text":"7.1 open the 'rc.local' using the following command - sudo vi /etc/rc.local 7.2 Add following command at the top of this file (preferably in the first line) - sudo docker run --privileged -d -v /home/pi/settings:/usr/src/app/settings pradeesi/rapi_cl_hc_sensor 7.3 Save the file using ' Esc + :wq ' command on vi editor. 7.4 Change the execution bit using the following command - sudo chmod +x /etc/rc.local 7.5 Reboot the Raspberry Pi using ' sudo reboot '. 7.6 After the reboot process gets completed, login back to the Raspberry Pi and check if the sensor container is running using the command ' sudo docker ps '. Note: If the docker container is not getting triggered on system startup, add 5 second delay by adding ' sleep 5 ' before the docker command added in the 'rc.local' file (in Step# 7.2).","title":"7. Configure Raspberry Pi to Trigger the Sensor Container at Startup:"},{"location":"basic_kubectl_cmds/","text":"kubectl version kubectl get nodes kubectl run --image= --port= Kubernetes Deployments: A Deployment controller provides declarative updates for Pods and ReplicaSets. You describe a desired state in a Deployment object, and the Deployment controller changes the actual state to the desired state at a controlled rate. You can define Deployments to create new ReplicaSets, or to remove existing Deployments and adopt all their resources with new Deployments. Create Deployment Use the following command to create a deployment using a yaml file - kubectl create -f yaml file path List Deployments: Use the following command to list all kubernetes deployments - kubectl get deployments Deployment Details: Use the following command to display the details of deployment - kubectl describe deployment Scale Deployment: Use the following command to scale (up/down) a kubernetes deployment - kubectl scale deployments/ deployment_name --replicas= number of replicas Delete Deployment: Use the following command to delete a deployment - kubectl delete deployment deployment_name Kubernetes Pods: A Pod represents a unit of deployment: a single instance of an application in Kubernetes, which might consist of either a single container or a small number of containers that are tightly coupled and that share resources. List Pods: Use the following command to list all the pods: kubectl get pods or use wide option to see more details - kubectl get pods -o wide List Pods Filter: You can filter the pods using the labels used in deployment - kubectl get pods -l label_name = label_value Pod Details: Use the following command to see the containers and used images for pods - kubectl describe pods Pod Logs: Use the following command to check the pod logs - kubectl logs pod_name Kubernetes Services: A Kubernetes Service is an abstraction which defines a logical set of Pods and a policy by which to access them - sometimes called a micro-service. List Service: Use the following command to list the current Services - kubectl get services You can filter the services using the labels used in deployment - kubectl get services -l label_name = label_value Create Service: Use the following command to create a new service - kubectl expose deployment/ deployment_name --type=\"NodePort\" --port port Service Details: Use the following command to find out what port was opened externally (by the NodePort option) - kubectl describe services/ service_name Delete Service: Use the following command to delete a service - kubectl delete service/ service_name or kubectl delete service -l label_name = label_value Parse Service Node Port: Use the following script to filterout the node-port of a service (change the service name) - export NODE_PORT=$(kubectl get services/ service-name -o go-template='{{(index .spec.ports 0).nodePort}}') echo NODE_PORT=$NODE_PORT Kubernetes Secrets: A Secret is an object that stores a piece of sensitive data like a password or key. List Secrets: Use the following command to list all secrets kubectl get secrets Secret Details: Use the following command to list the secret details - kubectl describe secrets/ secret_name Create Secret: Use the following command to create secret - kubectl create secret generic secret_name --from-literal= key_name = key_value Delete Secret: use the following command to delete a secret - kubectl delete secret secret_name Interacting with Pod Containers List Env Variables: Use the following command to list the environment variables - kubectl exec pod_name env Access Container Shell: Use the following command to access bash shell in a container - kubectl exec -ti pod_name bash Note: To close your container connection type ' exit '. ========================================= kubectl proxy curl http://localhost:8001/version export POD_NAME=$(kubectl get pods -o go-template --template '{{range .items}}{{.metadata.name}}{{\"\\n\"}}{{end}}') echo Name of the Pod: $POD_NAME curl http://localhost:8001/api/v1/namespaces/default/pods/$POD_NAME/proxy =========================================","title":"Basic kubectl cmds"},{"location":"basic_kubectl_cmds/#kubernetes-deployments","text":"A Deployment controller provides declarative updates for Pods and ReplicaSets. You describe a desired state in a Deployment object, and the Deployment controller changes the actual state to the desired state at a controlled rate. You can define Deployments to create new ReplicaSets, or to remove existing Deployments and adopt all their resources with new Deployments. Create Deployment Use the following command to create a deployment using a yaml file - kubectl create -f yaml file path List Deployments: Use the following command to list all kubernetes deployments - kubectl get deployments Deployment Details: Use the following command to display the details of deployment - kubectl describe deployment Scale Deployment: Use the following command to scale (up/down) a kubernetes deployment - kubectl scale deployments/ deployment_name --replicas= number of replicas Delete Deployment: Use the following command to delete a deployment - kubectl delete deployment deployment_name","title":"Kubernetes Deployments:"},{"location":"basic_kubectl_cmds/#kubernetes-pods","text":"A Pod represents a unit of deployment: a single instance of an application in Kubernetes, which might consist of either a single container or a small number of containers that are tightly coupled and that share resources. List Pods: Use the following command to list all the pods: kubectl get pods or use wide option to see more details - kubectl get pods -o wide List Pods Filter: You can filter the pods using the labels used in deployment - kubectl get pods -l label_name = label_value Pod Details: Use the following command to see the containers and used images for pods - kubectl describe pods Pod Logs: Use the following command to check the pod logs - kubectl logs pod_name","title":"Kubernetes Pods:"},{"location":"basic_kubectl_cmds/#kubernetes-services","text":"A Kubernetes Service is an abstraction which defines a logical set of Pods and a policy by which to access them - sometimes called a micro-service. List Service: Use the following command to list the current Services - kubectl get services You can filter the services using the labels used in deployment - kubectl get services -l label_name = label_value Create Service: Use the following command to create a new service - kubectl expose deployment/ deployment_name --type=\"NodePort\" --port port Service Details: Use the following command to find out what port was opened externally (by the NodePort option) - kubectl describe services/ service_name Delete Service: Use the following command to delete a service - kubectl delete service/ service_name or kubectl delete service -l label_name = label_value Parse Service Node Port: Use the following script to filterout the node-port of a service (change the service name) - export NODE_PORT=$(kubectl get services/ service-name -o go-template='{{(index .spec.ports 0).nodePort}}') echo NODE_PORT=$NODE_PORT","title":"Kubernetes Services:"},{"location":"basic_kubectl_cmds/#kubernetes-secrets","text":"A Secret is an object that stores a piece of sensitive data like a password or key. List Secrets: Use the following command to list all secrets kubectl get secrets Secret Details: Use the following command to list the secret details - kubectl describe secrets/ secret_name Create Secret: Use the following command to create secret - kubectl create secret generic secret_name --from-literal= key_name = key_value Delete Secret: use the following command to delete a secret - kubectl delete secret secret_name","title":"Kubernetes Secrets:"},{"location":"basic_kubectl_cmds/#interacting-with-pod-containers","text":"List Env Variables: Use the following command to list the environment variables - kubectl exec pod_name env Access Container Shell: Use the following command to access bash shell in a container - kubectl exec -ti pod_name bash Note: To close your container connection type ' exit '.","title":"Interacting with Pod Containers"},{"location":"basic_kubectl_cmds/#_1","text":"kubectl proxy curl http://localhost:8001/version export POD_NAME=$(kubectl get pods -o go-template --template '{{range .items}}{{.metadata.name}}{{\"\\n\"}}{{end}}') echo Name of the Pod: $POD_NAME curl http://localhost:8001/api/v1/namespaces/default/pods/$POD_NAME/proxy =========================================","title":"========================================="},{"location":"create_gke_engine/","text":"Create Kubernetes Cluster (GKE) on Google Cloud Open Kubernetes Engine: 2 3 4 5 6","title":"Create Kubernetes Cluster (GKE) on Google Cloud"},{"location":"create_gke_engine/#create-kubernetes-cluster-gke-on-google-cloud","text":"","title":"Create Kubernetes Cluster (GKE) on Google Cloud"},{"location":"create_gke_engine/#open-kubernetes-engine","text":"","title":"Open Kubernetes Engine:"},{"location":"create_gke_engine/#2","text":"","title":"2"},{"location":"create_gke_engine/#3","text":"","title":"3"},{"location":"create_gke_engine/#4","text":"","title":"4"},{"location":"create_gke_engine/#5","text":"","title":"5"},{"location":"create_gke_engine/#6","text":"","title":"6"},{"location":"deploy_backend/","text":"Deploy the Backend Application Components on CCP Kuberneted Cluster (Tenant Cluster) In this section you would deploy the backend components of the IoT Application on the Kubernetes cluster deployed on your CCP instance. Following diagram shows the high-level architure of these backend application containers - SSH into your kubernetes master node and start deploying the components by following the instruction given below - 1. Deploy MariaDB Databse: MariaDB will be used in the backend to save the sensor data recieved from AWS IoT platform over MQTT protocol. 1.1 Create Kubernetes Secret: A Secret is an object that contains a small amount of sensitive data such as a password, a token, or a key. Such information might otherwise be put in a Pod specification or in an image; putting it in a Secret object allows for more control over how it is used, and reduces the risk of accidental exposure. The MariaDB container image uses an environment varinable named as 'MYSQL_ROOT_PASSWORD', it hold the root password required to access the database. So you would create a new secret with 'password' key (value as 'cisco123') which would later be used in mariaDB deployment yaml file. 1.1.1: Create DB Password Secret - Use the following command to create a new secret on your kubernetes cluster - kubectl create secret generic mariadb-root-pass --from-literal=password=cisco123 1.1.2: Verify DB Password Secret - Check if the secret was created successfully or not - kubectl get secret mariadb-root-pass You should have the output similar to the following screenshot - 1.2 Create Persistent Volume Claim: A Persistent Volume Claim (PVC) is a request for storage by a user. It is similar to a pod. Pods consume node resources and PVCs consume Persistent Volume (PV) resources. Pods can request specific levels of resources (CPU and Memory). Claims can request specific size and access modes (e.g., can be mounted once read/write or many times read-only). To keep the sensor data safe during Pod restarts, you would create a new Persistent Volume Claim. 1.2.1: Create Persistent Volume Claim - Use the following command to create a new Persistent Volume Claim for MariaDB Pod - kubectl create -f https://raw.githubusercontent.com/pradeesi/HybridCloudApp/master/HybridCloudApp/Kubernetes/Backend/Mariadb/mariadb_persistent_volume.yaml 1.2.2: Verfiy Persistent Volume Claim - Check if the PVC was created successfully or not - kubectl get pvc mariadb-pv-claim You should have the output similar to the following screenshot - Note: It can take up to a few minutes for the PVs to be provisioned. 1.3 Deploy MariaDB: MariaDB is a community-developed fork of the MySQL relational database management system intended to remain free under the GNU GPL. Development is led by some of the original developers of MySQL, who forked it due to concerns over its acquisition by Oracle Corporation. MariaDB intends to maintain high compatibility with MySQL, ensuring a drop-in replacement capability with library binary parity and exact matching with MySQL APIs and commands. 1.3.1: Deploy MariaDB - Use the following command to create a MariaDB kubernetes deployment - kubectl create -f https://raw.githubusercontent.com/pradeesi/HybridCloudApp/master/HybridCloudApp/Kubernetes/Backend/Mariadb/mariadb_deployment.yaml 1.3.1: Check Deployment Status - Use the following command to check if the kubernetes deployment was successfully created or not - kubectl get deployment iot-backend-mariadb You should have the output similar to the following screenshot - 1.4 Create DB Service: Since the MariaDB will be accessed by other services like 'MQTT to DB Agent' and 'REST API Agent'; you need to expose it internally withing the kubernetes cluster using a Service. 1.4.1: Expose MariaDB to other Pods - Create a new kubernetes service using the following command - kubectl create -f https://raw.githubusercontent.com/pradeesi/HybridCloudApp/master/HybridCloudApp/Kubernetes/Backend/Mariadb/mariadb_service.yaml 1.4.2: Verify Service Status - Use the following command to check if the kubernetes service was deployed successfully or not - kubectl get service mariadb-service You should have the output similar to the following screenshot - 2. Deploy MQTT to DB Agent: 'MQTT to DB Agent' will subscribe to the MQTT Topic and listen to the incomming sensor data from AWS IoT platform. It will then parse the sensor data and insert it into the MariaDB. 2.1: Deploy MQTT to DB Agent - Use the following command to create mqtt-to-db-agent kubernetes deployment - kubectl create -f https://raw.githubusercontent.com/pradeesi/HybridCloudApp/master/HybridCloudApp/Kubernetes/Backend/MQTT_DB_Agent/mqtt_db_agent_deployment.yaml 2.2: Check Deployment Status - Use the following command to check if the kubernetes deployment was created successfully or not - kubectl get deployment iot-backend-mqtt-db-agent You should have the output similar to the following screenshot - 3. Deploy REST API Agent: The 'REST API Agent' would act as the gateway to the backend application. It will listen to the incomming HTTP requests from the frontend application that you will deploy on Google Cloud. 3.1 Deploy REST API Agent: 3.1.1: Deploy REST API Agent - Use the following command to create the rest-api-agent kubernetes deployment - kubectl create -f https://raw.githubusercontent.com/pradeesi/HybridCloudApp/master/HybridCloudApp/Kubernetes/Backend/REST_API_Agent/rest_api_agent.yaml 3.1.2: Check Deployment Status - Use the following command to check if the kubernetes deployment was created successfully or not - kubectl get deployment iot-backend-rest-api-agent You should have the output similar to the following screenshot - 3.2 Expose REST API Agent to Google Cloud using Kubernetes Service: Since the frontend app from Google Cloud would access the REST APIs exposed by the 'REST API Agent', you need to create a new kubernetes service for it. 3.2.1: Create REST API Agent NodePort Service - You can create a new kubernetes service using the following command - kubectl create -f https://raw.githubusercontent.com/pradeesi/HybridCloudApp/master/HybridCloudApp/Kubernetes/Backend/REST_API_Agent/rest_api_agent_service_node_port.yaml 3.2.2: Check REST API Agent Service Status - You can use the following command to check if the kubernetes service was creted successfully or not - kubectl get service rest-api-agent-service You should have the output similar to the following screenshot - 3.3 Locate the IP and Port to Access Node-Port Service: You need to find the NodePort and Kubernetes Node external IP to access the 'rest-api-agent. Use the following command to display the port exposed by 'rest-api-agent-service' - kubectl get service rest-api-agent-service Use the following command to display the 'External-IP' of you kubernetes nodes - kubectl get nodes -o wide Following screenshot highlights the Port and Node IPs in the command outputs - Important: Note down the Node External IP Address and NodePort Service Port Number. These values would be used in next section for deploying the frontend app as the environment variables values (' BACKEND_HOST ' and ' BACKEND_PORT '). 4 Test the REST API Agent Service: To test the REST API service try to access following url from your web browser (use the node's external ip and service port from the previous section # 3.3)- http:// node's external ip :30500/ If your REST API Agent is working properly, you should see 'Welcome to the API Service...!' message on your browser as shown in the following screenshot - Following are the other urls that you could test - http:// node's external ip :30500/cities http:// node's external ip :30500/temperature http:// node's external ip :30500/humidity http:// node's external ip :30500/sensor_data/city Next Steps: You have successfully deployed all the backend components of the iot-app on the CCP kubernetes cluster. Now you may proceed futher and deploy the frontend components on Google Cloud.","title":"Deploy the Backend Application Components on CCP Kuberneted Cluster (Tenant Cluster)"},{"location":"deploy_backend/#deploy-the-backend-application-components-on-ccp-kuberneted-cluster-tenant-cluster","text":"In this section you would deploy the backend components of the IoT Application on the Kubernetes cluster deployed on your CCP instance. Following diagram shows the high-level architure of these backend application containers - SSH into your kubernetes master node and start deploying the components by following the instruction given below -","title":"Deploy the Backend Application Components on CCP Kuberneted Cluster (Tenant Cluster)"},{"location":"deploy_backend/#1-deploy-mariadb-databse","text":"MariaDB will be used in the backend to save the sensor data recieved from AWS IoT platform over MQTT protocol.","title":"1. Deploy MariaDB Databse:"},{"location":"deploy_backend/#11-create-kubernetes-secret","text":"A Secret is an object that contains a small amount of sensitive data such as a password, a token, or a key. Such information might otherwise be put in a Pod specification or in an image; putting it in a Secret object allows for more control over how it is used, and reduces the risk of accidental exposure. The MariaDB container image uses an environment varinable named as 'MYSQL_ROOT_PASSWORD', it hold the root password required to access the database. So you would create a new secret with 'password' key (value as 'cisco123') which would later be used in mariaDB deployment yaml file. 1.1.1: Create DB Password Secret - Use the following command to create a new secret on your kubernetes cluster - kubectl create secret generic mariadb-root-pass --from-literal=password=cisco123 1.1.2: Verify DB Password Secret - Check if the secret was created successfully or not - kubectl get secret mariadb-root-pass You should have the output similar to the following screenshot -","title":"1.1 Create Kubernetes Secret:"},{"location":"deploy_backend/#12-create-persistent-volume-claim","text":"A Persistent Volume Claim (PVC) is a request for storage by a user. It is similar to a pod. Pods consume node resources and PVCs consume Persistent Volume (PV) resources. Pods can request specific levels of resources (CPU and Memory). Claims can request specific size and access modes (e.g., can be mounted once read/write or many times read-only). To keep the sensor data safe during Pod restarts, you would create a new Persistent Volume Claim. 1.2.1: Create Persistent Volume Claim - Use the following command to create a new Persistent Volume Claim for MariaDB Pod - kubectl create -f https://raw.githubusercontent.com/pradeesi/HybridCloudApp/master/HybridCloudApp/Kubernetes/Backend/Mariadb/mariadb_persistent_volume.yaml 1.2.2: Verfiy Persistent Volume Claim - Check if the PVC was created successfully or not - kubectl get pvc mariadb-pv-claim You should have the output similar to the following screenshot - Note: It can take up to a few minutes for the PVs to be provisioned.","title":"1.2 Create Persistent Volume Claim:"},{"location":"deploy_backend/#13-deploy-mariadb","text":"MariaDB is a community-developed fork of the MySQL relational database management system intended to remain free under the GNU GPL. Development is led by some of the original developers of MySQL, who forked it due to concerns over its acquisition by Oracle Corporation. MariaDB intends to maintain high compatibility with MySQL, ensuring a drop-in replacement capability with library binary parity and exact matching with MySQL APIs and commands. 1.3.1: Deploy MariaDB - Use the following command to create a MariaDB kubernetes deployment - kubectl create -f https://raw.githubusercontent.com/pradeesi/HybridCloudApp/master/HybridCloudApp/Kubernetes/Backend/Mariadb/mariadb_deployment.yaml 1.3.1: Check Deployment Status - Use the following command to check if the kubernetes deployment was successfully created or not - kubectl get deployment iot-backend-mariadb You should have the output similar to the following screenshot -","title":"1.3 Deploy MariaDB:"},{"location":"deploy_backend/#14-create-db-service","text":"Since the MariaDB will be accessed by other services like 'MQTT to DB Agent' and 'REST API Agent'; you need to expose it internally withing the kubernetes cluster using a Service. 1.4.1: Expose MariaDB to other Pods - Create a new kubernetes service using the following command - kubectl create -f https://raw.githubusercontent.com/pradeesi/HybridCloudApp/master/HybridCloudApp/Kubernetes/Backend/Mariadb/mariadb_service.yaml 1.4.2: Verify Service Status - Use the following command to check if the kubernetes service was deployed successfully or not - kubectl get service mariadb-service You should have the output similar to the following screenshot -","title":"1.4 Create DB Service:"},{"location":"deploy_backend/#2-deploy-mqtt-to-db-agent","text":"'MQTT to DB Agent' will subscribe to the MQTT Topic and listen to the incomming sensor data from AWS IoT platform. It will then parse the sensor data and insert it into the MariaDB. 2.1: Deploy MQTT to DB Agent - Use the following command to create mqtt-to-db-agent kubernetes deployment - kubectl create -f https://raw.githubusercontent.com/pradeesi/HybridCloudApp/master/HybridCloudApp/Kubernetes/Backend/MQTT_DB_Agent/mqtt_db_agent_deployment.yaml 2.2: Check Deployment Status - Use the following command to check if the kubernetes deployment was created successfully or not - kubectl get deployment iot-backend-mqtt-db-agent You should have the output similar to the following screenshot -","title":"2. Deploy MQTT to DB Agent:"},{"location":"deploy_backend/#3-deploy-rest-api-agent","text":"The 'REST API Agent' would act as the gateway to the backend application. It will listen to the incomming HTTP requests from the frontend application that you will deploy on Google Cloud.","title":"3. Deploy REST API Agent:"},{"location":"deploy_backend/#31-deploy-rest-api-agent","text":"3.1.1: Deploy REST API Agent - Use the following command to create the rest-api-agent kubernetes deployment - kubectl create -f https://raw.githubusercontent.com/pradeesi/HybridCloudApp/master/HybridCloudApp/Kubernetes/Backend/REST_API_Agent/rest_api_agent.yaml 3.1.2: Check Deployment Status - Use the following command to check if the kubernetes deployment was created successfully or not - kubectl get deployment iot-backend-rest-api-agent You should have the output similar to the following screenshot -","title":"3.1 Deploy REST API Agent:"},{"location":"deploy_backend/#32-expose-rest-api-agent-to-google-cloud-using-kubernetes-service","text":"Since the frontend app from Google Cloud would access the REST APIs exposed by the 'REST API Agent', you need to create a new kubernetes service for it. 3.2.1: Create REST API Agent NodePort Service - You can create a new kubernetes service using the following command - kubectl create -f https://raw.githubusercontent.com/pradeesi/HybridCloudApp/master/HybridCloudApp/Kubernetes/Backend/REST_API_Agent/rest_api_agent_service_node_port.yaml 3.2.2: Check REST API Agent Service Status - You can use the following command to check if the kubernetes service was creted successfully or not - kubectl get service rest-api-agent-service You should have the output similar to the following screenshot -","title":"3.2 Expose REST API Agent to Google Cloud using Kubernetes Service:"},{"location":"deploy_backend/#33-locate-the-ip-and-port-to-access-node-port-service","text":"You need to find the NodePort and Kubernetes Node external IP to access the 'rest-api-agent. Use the following command to display the port exposed by 'rest-api-agent-service' - kubectl get service rest-api-agent-service Use the following command to display the 'External-IP' of you kubernetes nodes - kubectl get nodes -o wide Following screenshot highlights the Port and Node IPs in the command outputs - Important: Note down the Node External IP Address and NodePort Service Port Number. These values would be used in next section for deploying the frontend app as the environment variables values (' BACKEND_HOST ' and ' BACKEND_PORT ').","title":"3.3 Locate the IP and Port to Access Node-Port Service:"},{"location":"deploy_backend/#4-test-the-rest-api-agent-service","text":"To test the REST API service try to access following url from your web browser (use the node's external ip and service port from the previous section # 3.3)- http:// node's external ip :30500/ If your REST API Agent is working properly, you should see 'Welcome to the API Service...!' message on your browser as shown in the following screenshot - Following are the other urls that you could test - http:// node's external ip :30500/cities http:// node's external ip :30500/temperature http:// node's external ip :30500/humidity http:// node's external ip :30500/sensor_data/city","title":"4 Test the REST API Agent Service:"},{"location":"deploy_backend/#next-steps","text":"You have successfully deployed all the backend components of the iot-app on the CCP kubernetes cluster. Now you may proceed futher and deploy the frontend components on Google Cloud.","title":"Next Steps:"},{"location":"deploy_frontend/","text":"Deploy the Frontend Application Components on Google Kubernetes Engine (GKE) In this section you would deploy the frontend components of the IoT Application on the Google Kubernetes Engine. Following diagram shows the high-level architure of these frontend application containers - 1. Login to Google Cloud Console and Open Kubernetes Engine: Login to Google Cloud Console using the credentials provided by the lab instructor. 2. Start Creating Deployment Definition: 2.1: Select the ' Workloads ' option on \" Google Cloud Console -- Kubernetes Engine \" page, and click on the ' Deploy ' button as shown in the following screenshot - 3. Add 'frontend_server' Container Image to the Deployment Definition: 3.1: Click on the ' Select Google Container Registry Image ' button on the 'Create Deployment' page as shown in the following screenshot - 3.2: Select the ' frontend_server ' container image from the pop-up window and clikc on the ' SELECT ' button as shown in the following screenshot - 4. Add Environment Variables to the 'frontend_server' Container: 4.1: Click on the '+ Add environment variable' button to add the environment variables for the 'frontend_server' container as shown in the following screenshot - 4.2: Add ' BACKEND_HOST ' and ' BACKEND_PORT ' variables as shown in the following screenshot (Use the values for 'BACKEND_HOST' and 'BACKEND_PORT' from the REST API Agent NodePort Service created earlier) - 5. Add Second Container Image ('nginx_srvr') to the Deployment Definition: 5.1: After clicking on the ' + Add Container ' button (shown in the previous screenshot), click again on the ' Select Google Container Registry image ' and select ' nginx_srvr ' image from the pop-up window. After selecting the image, click on the ' Done ' button as shown in the following screenshot - 6. Add Application Name, Select Cluster and Deploy the Application: 6.1: Verify that you have ' frontend_server ' and ' nginx_srvr ' container images selected on your screen. 6.2: Change the application name to ' iot-frontend-\\ user-#> ' and select the 'Cluster' from the drop down menu. Now you can click on the 'Deploy' button as shown in the following screenshot - Importnat: Application deployment may take some time. Wait for it's completion before proceeding with the next steps. 7. Expose the Application by Creating Kubernetes Service: 7.1: Click on your Workload 'Name' (Deployment) as shown in the following screenshot - 7.2: Click on the ' Deploy ' button as shown in the following screenshot - 7.3: In the 'New Port Mapping' set ' Port ' and ' Target Port ' as ' 80 ' and click on the ' Done ' button. Make sure the ' Service type ' is ' Load balancer ' and click on the ' Expose ' button as shown in the following screenshot - Important: Service deployment may take some time. Wait for it before proceeding with the next steps. 8. Open the Application Dashboard: 8.1: Go to ' Kubernetes Engine -- Services ' and cick on the ' Endpoints ' respective to your kubernetes service as shown in the following screenshot - 8.2: You should see the following webpage in the new tab of your browser. Click on the \"Open Dashboard\" button to open the application dashboard as shown in the following screenshot - 8.3: If you see the following web-page with charts filled with data, your application is working :-) Congratulations!!!","title":"Deploy the Frontend Application Components on Google Kubernetes Engine (GKE)"},{"location":"deploy_frontend/#deploy-the-frontend-application-components-on-google-kubernetes-engine-gke","text":"In this section you would deploy the frontend components of the IoT Application on the Google Kubernetes Engine. Following diagram shows the high-level architure of these frontend application containers -","title":"Deploy the Frontend Application Components on Google Kubernetes Engine (GKE)"},{"location":"deploy_frontend/#1-login-to-google-cloud-console-and-open-kubernetes-engine","text":"Login to Google Cloud Console using the credentials provided by the lab instructor.","title":"1. Login to Google Cloud Console and Open Kubernetes Engine:"},{"location":"deploy_frontend/#2-start-creating-deployment-definition","text":"2.1: Select the ' Workloads ' option on \" Google Cloud Console -- Kubernetes Engine \" page, and click on the ' Deploy ' button as shown in the following screenshot -","title":"2. Start Creating Deployment Definition:"},{"location":"deploy_frontend/#3-add-frontend_server-container-image-to-the-deployment-definition","text":"3.1: Click on the ' Select Google Container Registry Image ' button on the 'Create Deployment' page as shown in the following screenshot - 3.2: Select the ' frontend_server ' container image from the pop-up window and clikc on the ' SELECT ' button as shown in the following screenshot -","title":"3. Add 'frontend_server' Container Image to the Deployment Definition:"},{"location":"deploy_frontend/#4-add-environment-variables-to-the-frontend_server-container","text":"4.1: Click on the '+ Add environment variable' button to add the environment variables for the 'frontend_server' container as shown in the following screenshot - 4.2: Add ' BACKEND_HOST ' and ' BACKEND_PORT ' variables as shown in the following screenshot (Use the values for 'BACKEND_HOST' and 'BACKEND_PORT' from the REST API Agent NodePort Service created earlier) -","title":"4. Add Environment Variables to the 'frontend_server' Container:"},{"location":"deploy_frontend/#5-add-second-container-image-nginx_srvr-to-the-deployment-definition","text":"5.1: After clicking on the ' + Add Container ' button (shown in the previous screenshot), click again on the ' Select Google Container Registry image ' and select ' nginx_srvr ' image from the pop-up window. After selecting the image, click on the ' Done ' button as shown in the following screenshot -","title":"5. Add Second Container Image ('nginx_srvr') to the Deployment Definition:"},{"location":"deploy_frontend/#6-add-application-name-select-cluster-and-deploy-the-application","text":"6.1: Verify that you have ' frontend_server ' and ' nginx_srvr ' container images selected on your screen. 6.2: Change the application name to ' iot-frontend-\\ user-#> ' and select the 'Cluster' from the drop down menu. Now you can click on the 'Deploy' button as shown in the following screenshot - Importnat: Application deployment may take some time. Wait for it's completion before proceeding with the next steps.","title":"6. Add Application Name, Select Cluster and Deploy the Application:"},{"location":"deploy_frontend/#7-expose-the-application-by-creating-kubernetes-service","text":"7.1: Click on your Workload 'Name' (Deployment) as shown in the following screenshot - 7.2: Click on the ' Deploy ' button as shown in the following screenshot - 7.3: In the 'New Port Mapping' set ' Port ' and ' Target Port ' as ' 80 ' and click on the ' Done ' button. Make sure the ' Service type ' is ' Load balancer ' and click on the ' Expose ' button as shown in the following screenshot - Important: Service deployment may take some time. Wait for it before proceeding with the next steps.","title":"7. Expose the Application by Creating Kubernetes Service:"},{"location":"deploy_frontend/#8-open-the-application-dashboard","text":"8.1: Go to ' Kubernetes Engine -- Services ' and cick on the ' Endpoints ' respective to your kubernetes service as shown in the following screenshot - 8.2: You should see the following webpage in the new tab of your browser. Click on the \"Open Dashboard\" button to open the application dashboard as shown in the following screenshot - 8.3: If you see the following web-page with charts filled with data, your application is working :-) Congratulations!!!","title":"8. Open the Application Dashboard:"},{"location":"kubernetes_basics/","text":"Appendix - 3: Kubernetes Basic Docs 1. Install and Configure kubectl For information on \"kubectl\" installation and configuration use the following document - https://kubernetes.io/docs/tasks/tools/install-kubectl/ 2. Google Cloud Container Registry: Pushing and Pulling Container Images To push or pull the container images to/from Google Container Registry you can follow the following article - https://cloud.google.com/container-registry/docs/pushing-and-pulling Note: You need to login to your Google Cloud account before pushing the container image. You can do it using the following command - gcloud auth login If you have multiple projects, you can select a specific one using the following command - gcloud config set project Project_ID Kubernetes Cheat Sheet: https://kubernetes.io/docs/reference/kubectl/cheatsheet/","title":"Appendix - 3: Kubernetes Basic Docs"},{"location":"kubernetes_basics/#appendix-3-kubernetes-basic-docs","text":"","title":"Appendix - 3: Kubernetes Basic Docs"},{"location":"kubernetes_basics/#1-install-and-configure-kubectl","text":"For information on \"kubectl\" installation and configuration use the following document - https://kubernetes.io/docs/tasks/tools/install-kubectl/","title":"1. Install and Configure kubectl"},{"location":"kubernetes_basics/#2-google-cloud-container-registry-pushing-and-pulling-container-images","text":"To push or pull the container images to/from Google Container Registry you can follow the following article - https://cloud.google.com/container-registry/docs/pushing-and-pulling Note: You need to login to your Google Cloud account before pushing the container image. You can do it using the following command - gcloud auth login If you have multiple projects, you can select a specific one using the following command - gcloud config set project Project_ID","title":"2. Google Cloud Container Registry: Pushing and Pulling Container Images"},{"location":"kubernetes_basics/#kubernetes-cheat-sheet","text":"https://kubernetes.io/docs/reference/kubectl/cheatsheet/","title":"Kubernetes Cheat Sheet:"}]}